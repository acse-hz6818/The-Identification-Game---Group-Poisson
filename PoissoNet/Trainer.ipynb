{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install pycm livelossplot albumentations\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Decide whether to use albumentations augmentations. Regular torchvision transforms will be used otherwise\n",
    "use_albumentations = False\n",
    "\n",
    "# Choose which model to transfer\n",
    "models_available = [\"Googlenet\", \"Resnet18\", \"Resnet50\", \"Inception\"]\n",
    "model_to_train = models_available[3]\n",
    "inception_use_aux = True # If using inception, choose whether to use the auxiliary output for training\n",
    "\n",
    "# Decide how many layers of a transferred model to freeze\n",
    "layers_to_freeze = 5\n",
    "\n",
    "# Input size for inception is different to other models\n",
    "if (model_to_train == \"Inception\"):\n",
    "    input_size = 299\n",
    "else:\n",
    "    input_size = 224\n",
    "\n",
    "# Choose hyperparameters\n",
    "lr = 1e-2\n",
    "momentum = 0.4\n",
    "batch_size = 64\n",
    "test_batch_size = 100 # decrease test batch size to reduce RAM usage\n",
    "n_epochs = 30\n",
    "weight_decay = 1e-3\n",
    "\n",
    "# Change this to wherever data is stored\n",
    "train_directory = \"./train/\"\n",
    "test_directory = \"./test/\"\n",
    "\n",
    "# Choose a descriptive model save name\n",
    "model_save_name = 'Inception_frozen_some_no_albumentations.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # this allows us to evaluate our model at every iteration\n",
    "from sklearn.metrics import f1_score # this allows us to evaluate our validation accuracy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit # this allows us to create a random validation split\n",
    "\n",
    "# These imports help plot the convergence and create the confusion matrix\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "\n",
    "# These imports help us create models and datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# This allows me to create my own custom dataset\n",
    "from torch.utils.data import Dataset \n",
    "from torchvision.datasets.folder import *\n",
    "\n",
    "# This allows me to import pretrained models for transfer learning\n",
    "import torchvision.models as models\n",
    "\n",
    "# This allows me to do a number of transforms for data augmentation later on\n",
    "from torchvision.transforms import *\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "# These imports help us write the submission file\n",
    "import json, csv\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# We could use albumentations to perform data augmentation\n",
    "if (use_albumentations):\n",
    "    from albumentations import Compose\n",
    "    import albumentations.augmentations.transforms as transforms\n",
    "\n",
    "# This helps us keep a copy of model state dicts\n",
    "import copy\n",
    "\n",
    "# To display random images\n",
    "from random import randrange\n",
    "\n",
    "# Enable hardware acceleration\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda:3'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The means and stds of our entire training set, calculated in another file:\n",
    "means = [0.4802, 0.4481, 0.3975]\n",
    "stds = [0.2302, 0.2265, 0.2262]\n",
    "\n",
    "if (use_albumentations):\n",
    "    def apply_normalization(X):\n",
    "        for i in range(3): # Normalization needs to be channel-wise\n",
    "            X[:,:,:,i] /= 255. # X is of the form [number of samples, height, width, channels]\n",
    "            X[:,:,:,i] -= means[i]\n",
    "            X[:,:,:,i] /= stds[i]\n",
    "        return X\n",
    "    \n",
    "# If not using albumentations, we will use the normalize function of torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_albumentations):\n",
    "    # Rewritten to help separate data from labels\n",
    "    def my_make_dataset(directory, class_to_idx, extensions=None, is_valid_file=None):\n",
    "        data = [] # create an empty list that will then hold our data\n",
    "        targets = [] # create an empty list that will then hold our label\n",
    "        directory = os.path.expanduser(directory) # the path to the data\n",
    "        # these next lines are from the default make_dataset:\n",
    "        both_none = extensions is None and is_valid_file is None\n",
    "        both_something = extensions is not None and is_valid_file is not None\n",
    "        if both_none or both_something:\n",
    "            raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
    "        if extensions is not None:\n",
    "            def is_valid_file(x):\n",
    "                return has_file_allowed_extension(x, extensions)\n",
    "        for target_class in sorted(class_to_idx.keys()):\n",
    "            class_index = class_to_idx[target_class]\n",
    "            target_dir = os.path.join(directory, target_class)\n",
    "            if not os.path.isdir(target_dir):\n",
    "                continue\n",
    "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    if is_valid_file(path):\n",
    "                        item = path, class_index # regular make_dataset would then simply return this\n",
    "                        data.append(path) # instead, we add to our separate lists\n",
    "                        targets.append(class_index)\n",
    "        return data, targets # and return two items\n",
    "\n",
    "\n",
    "    # A custom dataset class that will work with albumentations. Only minor changes from regular ImageFolder:\n",
    "    class AlbumentationImageFolder(ImageFolder):\n",
    "        def __init__(self, root, extensions=IMG_EXTENSIONS, transform=None,\n",
    "                     target_transform=None, is_valid_file=None, augmentation=None):\n",
    "            super(ImageFolder, self).__init__(root, default_loader, IMG_EXTENSIONS if is_valid_file is None else None,\n",
    "                                              transform=transform,\n",
    "                                              target_transform=target_transform,\n",
    "                                              is_valid_file=is_valid_file)\n",
    "\n",
    "            classes, class_to_idx = self._find_classes(self.root)\n",
    "            data, targets = my_make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "            self.imgs = self.samples\n",
    "            # store data and targets separately:\n",
    "            self.data = [ToTensor()(self.loader(data[i])) for i in range(len(data))]\n",
    "            self.targets = targets\n",
    "            self.augmentation = augmentation # store augmentations to be applied\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path, target = self.samples[idx]\n",
    "            sample = self.loader(path)\n",
    "            if self.augmentation is not None:\n",
    "                sample = self.augmentation(image=np.asarray(sample))['image'] # apply augmentation\n",
    "                sample = np.transpose(sample, (2, 0, 1)).astype(np.float32) # get the right axis order\n",
    "                sample = torch.tensor(sample, dtype=torch.float) # transform into tensor\n",
    "\n",
    "            return sample, target\n",
    "    \n",
    "    # this will hold our training and validation datasets after separating them from my_data\n",
    "    # it is functionally the same as AlbumentationImageFolder except it takes in data and targets as inputs rather than a path\n",
    "    # This is so that we can pass it X_train/y_train and X_val/y_val after getting them with StratifiedShuffleSplit\n",
    "    class CustomImageTensorDataset(Dataset):\n",
    "        def __init__(self, data, targets, transform=None):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                data (Tensor): A tensor containing the data e.g. images\n",
    "                targets (Tensor): A tensor containing all the labels\n",
    "                transform (callable, optional): Optional transform to be applied\n",
    "                    on a sample.\n",
    "            \"\"\"\n",
    "            self.data = data\n",
    "            self.targets = targets\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            sample, label = self.data[idx], self.targets[idx]\n",
    "            if self.transform:\n",
    "                sample = ToPILImage()(sample).convert(\"RGB\")\n",
    "                sample = self.transform(image=np.array(sample, dtype = np.uint8))['image']\n",
    "                sample = np.transpose(sample, (2, 0, 1)).astype(np.float32)\n",
    "                sample = torch.tensor(sample, dtype=torch.float)\n",
    "\n",
    "            return sample, label\n",
    "    \n",
    "# A custom dataset class that retains filenames for use in creating the csv file\n",
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # We only need to override the __getitem__ method\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_albumentations):\n",
    "    my_data = AlbumentationImageFolder(train_directory) # Load all the data\n",
    "    \n",
    "    # Plot some of it to observe it:\n",
    "    fig, axarr = plt.subplots(5, 5, figsize=(8, 8))\n",
    "    for ax, j in zip(axarr.flatten(), range(25)):\n",
    "      i = randrange(100000)\n",
    "      ax.imshow(my_data.data[i].permute(1, 2, 0))\n",
    "      ax.set_title(my_data.targets[i])\n",
    "      ax.get_xaxis().set_visible(False) # turn off the axes to improve visibility\n",
    "      ax.get_yaxis().set_visible(False) # since the axes do not provide useful info\n",
    "    plt.subplots_adjust(hspace=0.3) # increase spaces between the plots to improve visibility\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a split that maintains the same % of images per class in the training and validation data\n",
    "    shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.05).split(my_data.data, my_data.targets)\n",
    "    indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n",
    "    \n",
    "    # Transform list of tensors into higher dimensional tensor\n",
    "    # This is to allow us to separate it by using indices[i] as an index\n",
    "    my_data.data = torch.stack(my_data.data)\n",
    "\n",
    "    # Separate training and validation data based on created indices\n",
    "    X_train, y_train = my_data.data[indices[0]].float(), torch.from_numpy(np.array(my_data.targets)[indices[0]])\n",
    "    X_val, y_val = my_data.data[indices[1]].float(),  torch.from_numpy(np.array(my_data.targets)[indices[1]])\n",
    "    \n",
    "    # Apply a number of transforms on the training data to augment our dataset\n",
    "    data_train = CustomImageTensorDataset(X_train, y_train.long(), transform=Compose([\n",
    "                        transforms.RandomContrast(),\n",
    "                        transforms.HorizontalFlip(),\n",
    "                        transforms.HueSaturationValue(),\n",
    "                        transforms.Resize(input_size, input_size),\n",
    "                        transforms.Normalize(means, stds)\n",
    "                        ]))\n",
    "    \n",
    "    # For validation, only resize and normalize the data\n",
    "    data_validate = CustomImageTensorDataset(X_val, y_val.long(), transform=Compose([transforms.Resize(input_size, input_size),\n",
    "                        transforms.Normalize(means, stds)]))\n",
    "else: # if not using albumentations\n",
    "    # Perform different set of augmentations that we found to be optimal\n",
    "    train_transform = Compose([\n",
    "        RandomApply([RandomChoice([RandomCrop(size=[64, 64], padding=6), RandomAffine(0, translate=(0.1, 0.1))])]),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomRotation(25),\n",
    "        Resize(input_size),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=means, std=stds), \n",
    "    ])\n",
    "\n",
    "    # Again only resize and normalize the data for validation and testing\n",
    "    validation_test_transform = Compose([\n",
    "        Resize(input_size),\n",
    "        ToTensor(),\n",
    "        Normalize(means, stds)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    if (model_to_train == \"Inception\"): # inception has built-in transforms\n",
    "        my_data = ImageFolder(train_directory, transform = validation_test_transform) # so we do not apply ours\n",
    "    else:\n",
    "        my_data = ImageFolder(train_directory, transform = train_transform) # otherwise do apply our transforms\n",
    "\n",
    "    # Use 95% of the data for training\n",
    "    train_size = int(0.95 * len(my_data))\n",
    "    validation_size = len(my_data) - train_size\n",
    "    # this is okay to use in place of Stratified Shuffle Split as we do not have a heavy imbalance in the number\n",
    "    # of items in each class\n",
    "    train_dataset, validation_dataset = random_split(my_data, [train_size, validation_size]) \n",
    "\n",
    "    \n",
    "# Create the loaders for all our tests\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "test_set = ImageFolderWithPaths(test_directory, transform = validation_test_transform)\n",
    "test_loader = DataLoader(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains our model on the training set\n",
    "def train(model, optimizer, criterion, data_loader):\n",
    "    model.train()\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if (inception_use_aux): # inception can use an auxiliary output to help learning\n",
    "            a2, aux = model(X.view(-1, 3, input_size, input_size))\n",
    "            loss1 = criterion(a2, y)\n",
    "            loss2 = criterion(aux, y) # aux also tries to predict the answer\n",
    "            loss = loss1 + 0.4 * loss2 # loss is a weighted sum\n",
    "        else:\n",
    "            a2 = model(X.view(-1, 3, input_size, input_size))\n",
    "            loss = criterion(a2, y)\n",
    "        loss.backward()\n",
    "        train_loss += loss*X.size(0)\n",
    "        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n",
    "        optimizer.step()  \n",
    "        \n",
    "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "# Checks our model against the validation set\n",
    "def validate(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    validation_loss, validation_accuracy = 0., 0.\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, input_size, input_size))\n",
    "            loss = criterion(a2, y)\n",
    "            validation_loss += loss*X.size(0)\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n",
    "            \n",
    "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n",
    "  \n",
    "# Evaluates our model's % accuracy\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    ys, y_preds = [], []\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, input_size, input_size))\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            ys.append(y.cpu().numpy())\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n",
    "\n",
    "# Generates predictions indexed with the right file names\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    files, y_preds = [], [] # files will hold the test image names\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            a2 = model(X.view(-1, 3, input_size, input_size))\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "\n",
    "# Generates predictions by averaging multiple models\n",
    "def multimodel_predict(models, data_loader):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = np.zeros(len(models))\n",
    "            for i, model in enumerate(models):\n",
    "                outputs[i] = model(X.view(-1, 3, input_size, input_size))\n",
    "            a2 = np.mean(outputs)\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a the requested model\n",
    "if (model_to_train == \"Googlenet\"):\n",
    "    model = models.googlenet(pretrained=True).to(device)\n",
    "elif (model_to_train == \"Resnet18\"):\n",
    "    model = models.resnet18(pretrained=True).to(device)\n",
    "elif (model_to_train == \"Resnet50\"):\n",
    "    model = models.resnet50(pretrained=True).to(device)\n",
    "elif (model_to_train == \"Inception\" and not inception_use_aux):\n",
    "    model = models.inception_v3(pretrained=True, aux_logits=False, transform_input=True).to(device)\n",
    "elif (model_to_train == \"Inception\"):\n",
    "    model = models.inception_v3(pretrained=True, transform_input=True).to(device)\n",
    "    # In this case only we need to also change the final layer of the aux output:\n",
    "    num_ftrs = model.AuxLogits.fc.in_features\n",
    "    model.AuxLogits.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# Change the last layer to have the right number of classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize at 0, and goes up to \"number of layers to freeze\"\n",
    "ct = 0\n",
    "\n",
    "for child in model.children():\n",
    "    ct += 1\n",
    "    if ct <= layers_to_freeze:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "print(ct) # Print how many layers in total there were, for info\n",
    "\n",
    "# No matter how many layers we froze, we want to at least unfreeze the last layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set these up to be updated while training, so that if we end up overfitting\n",
    "# We still save the best version of the model\n",
    "best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "best_acc = 0.0\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    logs = {}\n",
    "    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    logs['' + 'accuracy'] = train_accuracy.item()\n",
    "    \n",
    "    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "    logs['val_' + 'log loss'] = validation_loss.item()\n",
    "    logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "    \n",
    "    if validation_accuracy.item() > best_acc: # If we got a better validation accuracy, save it\n",
    "        best_acc = validation_accuracy.item()\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = F\"./{model_save_name}\" \n",
    "torch.save(best_model_wts, path) # save the version of the model with the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
