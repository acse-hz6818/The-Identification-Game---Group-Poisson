{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports help us create models and datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# This allows me to import pretrained models for transfer learning\n",
    "import torchvision.models as models\n",
    "\n",
    "# This allows me to do a number of transforms for data augmentation later on\n",
    "from torchvision.transforms import *\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "# This helps us write the output file\n",
    "import csv\n",
    "\n",
    "# Enable hardware acceleration\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda:3'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The means and stds of our entire training set, calculated in another file:\n",
    "means = [0.4802, 0.4481, 0.3975]\n",
    "stds = [0.2302, 0.2265, 0.2262]\n",
    "\n",
    "# Transforms to apply on inception models without TTA (Test-Time Augmentation)\n",
    "validation_test_transform_inception = Compose([\n",
    "            Resize(299),\n",
    "            ToTensor(),\n",
    "            Normalize(means, stds)\n",
    "        ])\n",
    "\n",
    "# Transforms to apply on inception models with TTA\n",
    "validation_test_transform_inception_tta = Compose([\n",
    "        # First make the images larger\n",
    "        Resize(299),\n",
    "        # For each image, predict both the original and horizontally flipped version\n",
    "        Lambda(lambda image: torch.stack([ToTensor()(RandomHorizontalFlip(p=1)(image)), \n",
    "                                          ToTensor()(image)])),\n",
    "        # Normalize every image\n",
    "        Lambda(lambda crops: torch.stack([normalize(crop, means, stds) for crop in crops])),\n",
    "    ])\n",
    "\n",
    "# Transforms to apply on non-inception models with no TTA\n",
    "validation_test_transform = Compose([\n",
    "            Resize(224),\n",
    "            ToTensor(),\n",
    "            Normalize(means, stds)\n",
    "        ])\n",
    "\n",
    "# Transforms to apply on non-inception models with TTA\n",
    "# (not used here, but available as a functionality)\n",
    "validation_test_transform_tta = Compose([\n",
    "        # First make the images larger\n",
    "        Resize(224),\n",
    "        # For each image, predict both the original and horizontally flipped version\n",
    "        Lambda(lambda image: torch.stack([ToTensor()(RandomHorizontalFlip(p=1)(image)), \n",
    "                                          ToTensor()(image)])),\n",
    "        # Normalize every image\n",
    "        Lambda(lambda crops: torch.stack([normalize(crop, means, stds) for crop in crops])),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # We only need to override the __getitem__ method\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "    \n",
    "image_folder = \"./test/\"\n",
    "    \n",
    "# For the prediction with 2 models:\n",
    "test_set = ImageFolderWithPaths(image_folder, transform = validation_test_transform)\n",
    "test_loader = DataLoader(test_set)\n",
    "\n",
    "test_set_inception = ImageFolderWithPaths(image_folder, transform = validation_test_transform_inception)\n",
    "test_loader_inception = DataLoader(test_set_inception)\n",
    "\n",
    "# For the prediction with just inception and TTA:\n",
    "test_set_tta = ImageFolderWithPaths(image_folder, transform = validation_test_transform_inception_tta)\n",
    "test_loader_tta = DataLoader(test_set_tta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions while applying test-time augmentation\n",
    "def predict_tta(model, data_loader):\n",
    "    model.eval()\n",
    "    files, y_preds = [], []\n",
    "    for X, y, z in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # There is a new dimension in each X - the number of transforms of each image\n",
    "            batch_size, n_crops, c, h, w = X.size()\n",
    "            X = X.view(-1, 3, 299, 299)\n",
    "            a2 = model(X)\n",
    "            a2 = a2.view(batch_size, n_crops, -1).mean(1) # Average out the predictions on every transform\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "            \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)\n",
    "\n",
    "# Predict by averaging the predictions of many models\n",
    "def multimodel_predict(model, inception_model, data_loader, inception_data_loader):\n",
    "    model.eval()\n",
    "    inception_model.eval()\n",
    "    files, y_preds = [], []\n",
    "    # This is necessary as model and inception model expect inputs from different loaders:\n",
    "    for (X, y, z), (Xi, yi, zi) in zip(data_loader, inception_data_loader):\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            Xi, yi = Xi.to(device), yi.to(device)\n",
    "            a0 = model(X.view(-1, 3, 224, 224)) # correct size for non-inception\n",
    "            a1 = inception_model(Xi.view(-1, 3, 299, 299)) # correct size for inception\n",
    "            a2 = (a0 + a1) / 2. # average out the predictions\n",
    "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            files.append(z)\n",
    "    \n",
    "    return np.concatenate(y_preds, 0), np.concatenate(files, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using TTA\n",
    "\n",
    "Run the following cells to predict with one inception model using Test-Time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The place where we stored the model we used with TTA:\n",
    "model_path = \"../Models/Inceptionv3_5_layers_frozen_horizontal_flip.pth\"\n",
    "\n",
    "# This model had aux outputs enable\n",
    "model = models.inception_v3(pretrained=True, aux_logits=True, transform_input=True)\n",
    "    \n",
    "# Change number of classes in aux and regular outputs:\n",
    "num_ftrs = model.AuxLogits.fc.in_features\n",
    "model.AuxLogits.fc = nn.Linear(num_ftrs, 200)    \n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# Load the weights of our trained model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Send the model to cuda if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictions, my_files = predict_tta(model, test_loader_tta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Multi-model\n",
    "\n",
    "Run the following cells to predict using the average of 2 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_path = \"../Models/Models/inceptionv3_5_layers_frozen_full_training_set_default_augmentations.pth\"\n",
    "model2_path = \"../Models/Resnet_no_layers_frozen_augmented_cropped_full_training_set.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of inception was trained without auxiliary outputs:\n",
    "model1 = models.inception_v3(pretrained=True, aux_logits=False, transform_input=True).to(device)\n",
    "    \n",
    "# Only need to change number of classes in the last layer\n",
    "num_ftrs = model1.fc.in_features\n",
    "model1.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# Load our saved weights\n",
    "model1.load_state_dict(torch.load(model1_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pre-trained version of resnet18\n",
    "model2 = models.resnet18()\n",
    "\n",
    "# Change the number of classes in the output\n",
    "num_ftrs = model2.fc.in_features\n",
    "model2.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# Load our learned weights\n",
    "model2.load_state_dict(torch.load(model2_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_predictions, my_files = multimodel_predict(model2, model1, test_loader, test_loader_inception)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the submission file\n",
    "\n",
    "Regardless of how the predictions were generated, this cell will save them in a file in the format: Filename,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .split('/') splits the path by folders, -1 chooses the file name\n",
    "# we use .lower to have the format as jpeg instead of JPEG\n",
    "my_files_clean = [my_files[i].split('/')[-1].lower() for i in range(len(my_files))]\n",
    "\n",
    "# Open a file and write it in csv format\n",
    "with open('submission_1.csv', 'w', encoding=\"ISO-8859-1\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow((\"Filename\", \"Label\"))\n",
    "    wr.writerows(zip(my_files_clean, my_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
